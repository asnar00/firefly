ᕦ(ツ)ᕤ
# engine

Just thinking about LLMs and nets in general, here's some architectural ideas.

## services layer

- the user is in charge of a hierarchy of devices and interfaces, from eyeglasses to watch to headset to phone to laptop to workstation to server to cloud

- "closer" devices provide better latency and availability, but less capability (eg. the internet might go down, cutting us off from the server, and we just have to do our best).

- this in turn means that when we think of a "neural net" or "AI" service (let's call them "services") we need to think of something that runs seamlessly on all these levels

- we store a list of running services for each level of the chain (eg. "mobile", "laptop", "server", "cloud"). For instance we may store 

    "laptop" : [ "llama7b" ],
    "server" : [ "llama70b" ],
    "cloud" : [ "openAI", "anthropic", "google" ]

- when a request is made, it's handed to all running services immediately. Any results handed back to the user are always stamped with the "quality level" attached to it"

- within a single level (eg. with three services at cloud level), multiple responses are used to sanity-check each other - if there's an outlier, we can flag this. Also, of course, if one of those services goes down or gets slow, the others keep running.

- cloud results are assumed to be the highest quality, and can be used to improve results from the laptop and server layers; using prompt engineering (just use previous examples of cloud responses as examples) and training.

- we can imagine an automated system that creates and manages local "microservices" that optimise themselves for specific user tasks. These microservices can be small nets specialised to certain tasks, or just straight compiled functions.

- from the outside, we make a Request, and get back a Result; the difference is that the actual value of Result can change over time, gradually getting better. The effect is equivalent to a web image gradually becoming sharper as higher-resolution versions are loaded.

## indexing system

- when we drop some document into the system (could be a single code snippet, or a manual, or whatever), we want it indexed semantically

- vector databases are implemented as services in the scheme above

- for any value, we can check trivially if it has changed

- for any pair of values, we can identify the best version of the data available in both values, and compare / operate on those

## agent layer

- an agent is just some piece of code that takes some input files, issues a bunch of LLM service requests, and spits out some output files

- agents can be set up to watch arrays of files, re-processing any that change

- changing the agent code will also cause every file to be re-processed

- some agents can be set to auto-run, others can be triggered

- a scheduler makes sure we minimise cost and latency while maximising quality


______________________________________________-
What are we doing tomorrow?

- set up a new server for miso2. In python.
- set up a new client for miso2. In typescript.
- get console up and running.

Then let's get this services architecture running.

Import has to happen; it will modify the 'code' part.
This will then invalidate a bunch of prompts; because each one will have a specific set of files it's looking at. 

Yup, so let's just get that little piggy working. This will be ideal because we will get the best kind of response from it.

Or you know what fuck it don't do the enginey thing next.

What's the most efficient way forward?

1- Get the card storage thing going
2- Use a datestamp-based input => output prompt running thingy
3- Build the UI now with the idea of values that refine over time


